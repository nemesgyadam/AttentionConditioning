{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import os, yaml\n",
    "from easydict import EasyDict\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from dataloader.bci_compet import get_dataset\n",
    "from dataloader.bci_compet import BCICompet2aIV\n",
    "\n",
    "from model.litmodel import get_litmodel\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "\n",
    "from utils.setup_utils import (\n",
    "    get_device,\n",
    "    get_log_name,\n",
    ")\n",
    "from utils.training_utils import get_callbacks\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_ROOT = 'cache'\n",
    "\n",
    "config_name = 'bcicompet2a_config'\n",
    "\n",
    "with open(f'configs/{config_name}.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    args = EasyDict(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(args, return_subject_id=False):\n",
    "    datasets = {}\n",
    "    for subject_id in range(0,9):\n",
    "        args['target_subject'] = subject_id\n",
    "        datasets[subject_id] = BCICompet2aIV(args)\n",
    "    return datasets\n",
    "\n",
    "path = os.path.join(CACHE_ROOT, f'{config_name}_base.pkl')\n",
    "\n",
    "if not os.path.isfile(path):\n",
    "    print('Cache miss, generating cache')\n",
    "    datasets = load_dataset(args)\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(datasets, file)\n",
    "else:\n",
    "    print('Loading cache')\n",
    "    with open(path, 'rb') as file:\n",
    "        datasets = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id in datasets.keys(): \n",
    "    print(f\"Subject {subject_id} has {len(datasets[subject_id])} trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id in datasets.keys(): \n",
    "    datasets[subject_id].return_subject_info = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 240\n",
    "val_size = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for LOS in datasets.keys():\n",
    "    name = 'baseline_L1SO_'+str(LOS)\n",
    "    args.VERSION = f'{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}-{name}'\n",
    "\n",
    "\n",
    "    #### Set Log ####\n",
    "    args['current_time'] = datetime.now().strftime('%Y%m%d')\n",
    "    args['LOG_NAME'] = get_log_name(args)\n",
    "\n",
    "    #### Update configs ####\n",
    "    if args.downsampling != 0: args['sampling_rate'] = args.downsampling\n",
    "    seed_everything(args.SEED)\n",
    "\n",
    "\n",
    "    train_datasets = {}\n",
    "    val_datasets = {}\n",
    "    for subject_id in datasets.keys():\n",
    "        if subject_id != LOS:\n",
    "            train_datasets[subject_id] = torch.utils.data.Subset(datasets[subject_id], range(train_size))\n",
    "            val_datasets[subject_id] = torch.utils.data.Subset(datasets[subject_id], range(train_size, train_size+val_size))\n",
    "\n",
    "\n",
    "    train_dataset_all = torch.utils.data.ConcatDataset(list(train_datasets.values()))\n",
    "    val_dataset_all = torch.utils.data.ConcatDataset(list(val_datasets.values()))\n",
    "    print(len(train_dataset_all), len(val_dataset_all))\n",
    "    \n",
    "    train_dataloader_all = DataLoader(train_dataset_all, batch_size=args['batch_size'], shuffle=True, num_workers=0, persistent_workers=False)\n",
    "    val_dataloader_all = DataLoader(val_dataset_all, batch_size=args['batch_size'], shuffle=False, num_workers=0, persistent_workers=False)\n",
    "\n",
    "    model = get_litmodel(args)\n",
    "    logger = TensorBoardLogger(args.LOG_PATH, \n",
    "                                    name=args.VERSION)\n",
    "    callbacks = get_callbacks(monitor='val_loss', args=args)\n",
    "\n",
    "    trainer = Trainer(\n",
    "            max_epochs=args['EPOCHS'],\n",
    "            callbacks=callbacks,\n",
    "            default_root_dir=args.CKPT_PATH,\n",
    "            logger=logger,\n",
    "            enable_progress_bar=False\n",
    "        )\n",
    "    trainer.fit(model,\n",
    "            train_dataloaders=train_dataloader_all,\n",
    "            val_dataloaders=val_dataloader_all)\n",
    "        \n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
